# 학습원리 {#principle}

### Inversion problem {-}

베이지안 기법 $p(\theta \mid X) = \dfrac{p(X \mid \theta) p(\theta)}{\int p(X \mid \theta) p(\theta) d\theta}$

### Intractability {-}

이론은 이론이고 실제로 저거 계산 못함.
exact, grid, approximation 이렇게 있는데 당신의 문제는 approximation 만 있다고 봐야.
그래서 근사 기법을 활용해야 함.
Conditioning 과 Optimisation 이 있음

### Conditioning vs Optimisation {-}

각각 긍정적인 예측과 부정적인 예측의 차이

다르게 이해할 수 있습니다.

데이터에 모델을 훈련 condition 시키느냐 이건 condition
데이터에 모델을 최대한 끼워넣느냐 이건 Optimisation
이 접근 방법은 완전히 다릅니다. 장단점 존재.

### Conditioning {-}

혹은 fully bayesian

Sampling (MCMC) and analytic proxy (VB)

### Optimisation {-}

MAP 계열

Gradient descent (GD)

Stochastic gradient descent (SGD)
